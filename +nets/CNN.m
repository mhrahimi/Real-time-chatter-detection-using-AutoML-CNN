function [layers, options] = CNN(numLabels, valImgs)
%% CNN modification
net = alexnet;
layers = net.Layers;

layers(end-2) = fullyConnectedLayer(numLabels);
layers(end) = classificationLayer;

%% Training Options
% InitialLearnRate = 0.000005;
% solverName = 'adam';
% options = trainingOptions(solverName, ...
%     'InitialLearnRate',InitialLearnRate, ...
%     'Plots','training-progress', ...
%     'LearnRateSchedule','piecewise', ...
%     'Shuffle', 'once',...
%     'LearnRateDropPeriod',15, ...
%     'LearnRateDropFactor',0.05, ...
%     'ValidationData', valImgs);


InitialLearnRate = 0.00005;
solverName = 'adam';
options = trainingOptions(solverName, ...
    'InitialLearnRate',InitialLearnRate, ...
    'Plots','training-progress', ...
    'LearnRateSchedule','piecewise', ...
    'Shuffle', 'once',...
    'MaxEpochs', 15, ...
    'LearnRateDropPeriod',3, ...
    'LearnRateDropFactor',0.005, ...
    'ValidationData', valImgs);

% % adam
% InitialLearnRate = 0.00001;
% solverName = 'adam';
% options = trainingOptions(solverName, ...
%     'InitialLearnRate',InitialLearnRate, ...
%     'Plots','training-progress', ...
%     'LearnRateSchedule','piecewise', ...
%     'Shuffle', 'once',...
%     'LearnRateDropPeriod',5, ...
%     'LearnRateDropFactor',0.0001, ...
%     'ValidationData', valImgs);

% stochastic gradient descent with momentum
% InitialLearnRate = 0.000005;
% solverName = 'sgdm';
% options = trainingOptions(solverName, ...
%     'InitialLearnRate',InitialLearnRate, ...
%     'Plots','training-progress', ...
%     'LearnRateSchedule','piecewise', ...
%     'Shuffle', 'once',...
%     'LearnRateDropPeriod',15, ...
%     'LearnRateDropFactor',0.0001, ...
%     'ValidationData', valImgs);
end